{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "'''\n",
    "    FileName      ：CNNandLabel.ipynb\n",
    "    Author        ：@zch0423\n",
    "    Date          ：Jun 25, 2021\n",
    "    Description   ：\n",
    "'''\n",
    "import pickle\n",
    "import gensim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_path = \"/Users/zch/Desktop/IM319_NLP.nosync/hw/pre_loaded/glove_twitter_200.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(w2v_path, \"rb\") as f:\n",
    "    pre_trained_w2v = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(path):\n",
    "    '''\n",
    "    @Description\n",
    "    返回文本\n",
    "    ------------\n",
    "    @Params\n",
    "    path, str\n",
    "    ------------\n",
    "    @Returns\n",
    "    X\n",
    "    '''\n",
    "    return pd.read_csv(path, index_col=0).post.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import initializers\n",
    "# 结果可复现\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0) \n",
    "def transform_feature(X_train, X_valid, X_test, MAX_LENGTH, pre_trained_w2v, dim_embedding):\n",
    "    '''\n",
    "    @Description\n",
    "    返回等长句子和embedding\n",
    "    ------------\n",
    "    @Params\n",
    "    X_train, X_valid, X_test, 文本数据\n",
    "    MAX_LENGTH, int, 最长长度\n",
    "    pre_trained_w2v, 预训练模型\n",
    "    dim_embedding, int\n",
    "    ------------\n",
    "    @Returns\n",
    "    X_train, X_valid, X_test, index2emb\n",
    "    '''\n",
    "    tokenizer = Tokenizer(filters='', lower=True) # 考虑标点符号，忽略大小写\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    #word2index = tokenizer.word_index\n",
    "    index2word = tokenizer.index_word # 从1开始，0预留给了 <pad>\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_valid = tokenizer.texts_to_sequences(X_valid)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    # embedding\n",
    "    index2emb = [np.zeros(dim_embedding)] # index 0 means <pad>\n",
    "\n",
    "    for i in range(1, len(index2word) + 1):\n",
    "        if index2word[i] in pre_trained_w2v:\n",
    "            index2emb.append(pre_trained_w2v[index2word[i]])\n",
    "        else:\n",
    "            index2emb.append(np.random.uniform(-0.05, 0.05, dim_embedding))\n",
    "    index2emb = np.array(index2emb)\n",
    "    max_length = min(max(len(x) for x in X_train), MAX_LENGTH)\n",
    "    X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
    "    X_valid = pad_sequences(X_valid, maxlen=max_length, padding='post')\n",
    "    X_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
    "    return X_train, X_valid, X_test, index2emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/zch/Desktop/IM319_NLP.nosync/project/rawData/preprocessed/SBIC.v2.%s.csv\"\n",
    "X_valid = loadData(path%\"dev\")\n",
    "X_train = loadData(path%\"trn\")\n",
    "X_test = loadData(path%\"tst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 60\n",
    "dim_embedding = len(pre_trained_w2v[\"the\"])\n",
    "X_train, X_valid, X_test, index2emb = transform_feature(X_train, X_valid, X_test, MAX_LENGTH, pre_trained_w2v, dim_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"/Users/zch/Desktop/IM319_NLP.nosync/project/data/CNN/\"\n",
    "np.save(out_dir+\"X_train.npy\", X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(out_dir+\"X_valid.npy\", X_valid)\n",
    "np.save(out_dir+\"X_test.npy\", X_test)\n",
    "np.save(out_dir+\"index2emb.npy\", index2emb)"
   ]
  },
  {
   "source": [
    "### 处理y标签\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = \"/Users/zch/Desktop/IM319_NLP.nosync/project/data/labels/%s.npy\"\n",
    "dev_label = np.load(label_path%\"dev\")\n",
    "trn_label = np.load(label_path%\"trn\")\n",
    "tst_label = np.load(label_path%\"tst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def refineLabel(labels):\n",
    "    def foo(z: float):\n",
    "        if np.isnan(z):\n",
    "            return 0\n",
    "        return 1 if z>0 else z\n",
    "\n",
    "    def foo2(z: float):\n",
    "        if np.isnan(z):\n",
    "            return 0\n",
    "        return 1 if z==0 else z\n",
    "    l = []\n",
    "    weights = []\n",
    "    for i in range(len(labels)):\n",
    "        l.append(list(map(foo, labels[i])))\n",
    "        weights.append(list(map(foo2, labels[i])))\n",
    "    return np.array(l), np.array(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_l, dev_w = refineLabel(dev_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveLabelandWeight(labels, weights, name=\"dev\"):\n",
    "    out_path = \"/Users/zch/Desktop/IM319_NLP.nosync/project/data/CNN/\"\n",
    "    np.save(out_path+name+\"_labels.npy\", labels)\n",
    "    np.save(out_path+name+\"_w.npy\", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveLabelandWeight(dev_l, dev_w, name=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveLabelandWeight(*refineLabel(trn_label), name=\"trn\")\n",
    "# saveLabelandWeight(*refineLabel(tst_label), name=\"tst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.load(\"/Users/zch/Desktop/IM319_NLP.nosync/project/data/CNN/trn_w.npy\")"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}